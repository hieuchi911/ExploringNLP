{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load qasModel2.py\n",
    "\"\"\"\n",
    "Created on Tue Feb 25 21:40:27 2020\n",
    "\n",
    "@author: Admin\n",
    "\"\"\"\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Feb 11 13:34:14 2020\n",
    "\n",
    "@author: Admin\n",
    "path to train file: \"Training and testing data\\\\light-training-data.json\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from word2Vec import word2vecClass\n",
    "from nltk.tokenize import word_tokenize\n",
    "from similarity import Similarity\n",
    "from context2query import Context2Query\n",
    "from query2context import Query2Context\n",
    "from megamerge import MegaMerge\n",
    "from outputlayer import OutputLayer\n",
    "\n",
    "class MyQuestionAnsweringModel():\n",
    "    \n",
    "    def __init__(self, max_context_length, max_query_length):\n",
    "        self.max_context_length = max_context_length\n",
    "        self.max_query_length = max_query_length\n",
    "        self.settings = {'window_size': 2, 'n': 100, 'epochs': 5, 'learning_rate': 0.001}\n",
    "        \n",
    "        self.extract_training_inputs(\"Training and testing data\\\\light-training-data.json\")\n",
    "        \n",
    "        input_h = tf.keras.Input(shape = (self.max_context_length, self.settings[\"n\"]), dtype = \"float32\", name = \"context_input\")\n",
    "        input_u = tf.keras.Input(shape = (self.max_query_length, self.settings[\"n\"]), dtype = \"float32\", name = \"query_input\")\n",
    "        \n",
    "        # LSTM layer for Context and for Query:\n",
    "        lstm_context = tf.keras.layers.LSTM(self.settings[\"n\"], activation = \"tanh\", trainable = False, return_sequences = True) # Define an LSTM LAYER for context matrix of size dxT\n",
    "        bidirectional_context_layer = tf.keras.layers.Bidirectional(lstm_context) # Define BiLSTM LAYER by wrapping the lstm_context LAYER\n",
    "        bidirectional_context_layer_tensor = bidirectional_context_layer(input_h) # context TENSOR of size (1, T, 2d) is returned by plugging an Input tensor context_inputs\n",
    "        #bidirectional_context_layer_tensor = bidirectional_context_layer(in_h) # context TENSOR of size (1, T, 2d) is returned by plugging an Input tensor context_inputs\n",
    "        \n",
    "        \n",
    "        lstm_query = tf.keras.layers.LSTM(self.settings[\"n\"], activation = \"tanh\", trainable = False, return_sequences = True) # Define an LSTM layer for query matrix of size dxT\n",
    "        bidirectional_query_layer = tf.keras.layers.Bidirectional(lstm_query) # Define BiLSTM LAYER by wrapping the lstm_query LAYER\n",
    "        bidirectional_query_layer_tensor = bidirectional_query_layer(input_u) # query TENSOR of size (1, J, 2d) is returned by plugging an Input tensor query_inputs\n",
    "        #bidirectional_query_layer_tensor = bidirectional_query_layer(in_u) # query TENSOR of size (1, J, 2d) is returned by plugging an Input tensor query_inputs\n",
    "        \n",
    "        #\n",
    "        #\n",
    "        #_____________________________ FORMING SIMILARITY MATRIX S _____________________________\n",
    "        \n",
    "        # Initiate a 1x6d trainable weight vector with random weights. The shape is 1x6d since this vector will be used in multiplication with concatenated version of outputs from Context (H) and Query (U) biLSTMs: S = alpha(H, U)\n",
    "        h = bidirectional_context_layer_tensor # Context TENSOR H with shape (1, num_words, features)\n",
    "        u = bidirectional_query_layer_tensor # Query TENSOR U with shape (1, num_words, features)\n",
    "        inputs = {\"Context\": h, \"Query\": u}\n",
    "        similarity_matrix = Similarity()(inputs)\n",
    "        \n",
    "        context2query = Context2Query()(u, similarity_matrix)\n",
    "        \n",
    "        query2context = Query2Context()(h, similarity_matrix)\n",
    "        \n",
    "        megamerge = MegaMerge()(h, context2query, query2context, self.max_context_length)\n",
    "        #\n",
    "        #\n",
    "        #_____________________________ MODELING LAYER _____________________________\n",
    "        \n",
    "        G = tf.expand_dims(tf.transpose(megamerge, (1, 0)), 0)  # Transpose G to shape (batch_size, timesteps = num_words, features = 800), then expand one more dimension\n",
    "                                                        # 0 to plug in the LSTM layer, which corresponds to the batch_size\n",
    "        lstm_m1 = tf.keras.layers.LSTM(self.settings[\"n\"], activation = \"tanh\", trainable = False, return_sequences = True) # Define an LSTM LAYER for M1\n",
    "        bidirectional_m1_layer = tf.keras.layers.Bidirectional(lstm_m1) # Define BiLSTM LAYER by wrapping the lstm_m1 LAYER\n",
    "        m1_tensor = bidirectional_m1_layer(G) # M1 TENSOR of size (1, T, 2d) is returned by plugging an Input tensor G\n",
    "        \n",
    "        \n",
    "        lstm_m2 = tf.keras.layers.LSTM(self.settings[\"n\"], activation = \"tanh\", trainable = False, return_sequences = True) # Define an LSTM LAYER for M1\n",
    "        bidirectional_m2_layer = tf.keras.layers.Bidirectional(lstm_m2) # Define BiLSTM LAYER by wrapping the lstm_m1 LAYER\n",
    "        m2_tensor = bidirectional_m2_layer(m1_tensor) # M2 TENSOR of size (1, T, 2d) is returned by plugging an Input tensor m1_tensor\n",
    "        #\n",
    "        #\n",
    "        #_____________________________ OUTPUT LAYER _____________________________\n",
    "        \n",
    "        # Discharge the first dimension from G, M1 and M2 because they won't be used anymore. Their shape will be (T, 8d), (T, 2d) and (T, 2d) respectively. We next transpose them to\n",
    "        # coherent shape of (8d, T) and (2d, T)\n",
    "        G = tf.transpose(G[0], (1, 0))\n",
    "        m1_tensor = tf.transpose(m1_tensor[0], (1, 0))\n",
    "        m2_tensor = tf.transpose(m2_tensor[0], (1, 0))\n",
    "        G_M1 = tf.transpose(tf.concat((G, m1_tensor), 0), (1, 0)) # G_M1 was of shape (10d, T) then transposed to (T, 10d)\n",
    "        \n",
    "        G_M2 = tf.transpose(tf.concat((G, m2_tensor), 0), (1, 0)) # G_M2 was of shape (10d, T) then transposed to (T, 10d)\n",
    "        \n",
    "        input_G = {\"G_M1\": G_M1, \"G_M2\": G_M2}\n",
    "        start_end_index_pred = OutputLayer(name = \"output_indices\")(input_G)\n",
    "        \n",
    "        model = tf.keras.Model([input_h, input_u], [start_end_index_pred])\n",
    "        adam = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "        \n",
    "        model.compile(loss = some_loss_function, optimizer = adam, metrics = [\"accuracy\"])\n",
    "        \n",
    "        self.model = model\n",
    "\n",
    "    def tokenizeCorpus(self, corpus):\n",
    "        corpusWithNoPunctuals = []\n",
    "        tokenized = word_tokenize(corpus)\n",
    "        weirdlist = [\".\", \",\", \"'\",\"\\\"\", \"!\", \"?\", \"'\", \"-\", \"[\", \"]\", \":\", \"''\", \"``\", \")\", \"(\"]\n",
    "        for word in tokenized:\n",
    "            if word not in weirdlist:\n",
    "                corpusWithNoPunctuals.append(word)\n",
    "            else:\n",
    "                continue\n",
    "        return corpusWithNoPunctuals\n",
    "    def extract_training_inputs(self, path):\n",
    "        # Define inputs to the model: word embeddings for Context and word embeddings for Query\n",
    "        with open(path) as f:\n",
    "            data = json.loads(f.read())\n",
    "        inputContext = []\n",
    "        ContextCorpus = \"\" # This contains all contexts in training file\n",
    "        inputQuery = []\n",
    "        QueryCorpus = \"\" # This contains all queries in training file\n",
    "        Answer = []\n",
    "        \n",
    "        \n",
    "        # Now create data to pass as input to the first BiLSTM layer of our model\n",
    "        # Concatenate all contexts to create word2vec vector:\n",
    "        a = 0\n",
    "        for context in data[\"data\"][0][\"paragraphs\"][0:2]:\n",
    "            if a==1:\n",
    "                break\n",
    "            ContextCorpus += \" \" + context[\"context\"]\n",
    "            for query in context[\"qas\"]:\n",
    "                QueryCorpus += \" \" + query[\"question\"]\n",
    "                Answer += query[\"answers\"]\n",
    "                if a == 3:\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "                a+=1\n",
    "        print(\"_______________________CONTEXTS__________________________ \\n\\n\" + ContextCorpus)\n",
    "        \n",
    "        print(\"_______________________QUERIES__________________________ \\n\\n\" + QueryCorpus)\n",
    "        \n",
    "        self.wordEmbeddingQuery = word2vecClass(self.settings, QueryCorpus)\n",
    "        self.wordEmbeddingQuery.tokenizeCorpus()\n",
    "        self.wordEmbeddingQuery.buildVocabulary()\n",
    "        self.wordEmbeddingQuery.train(self.wordEmbeddingQuery.generate_training_data())\n",
    "        # wordEmbeddingQuery.w1 now contains num_word row vectors, each of which has a size of settings[\"n\"]\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.wordEmbeddingContext = word2vecClass(self.settings, ContextCorpus)\n",
    "        self.wordEmbeddingContext.tokenizeCorpus()\n",
    "        self.wordEmbeddingContext.buildVocabulary()\n",
    "        self.wordEmbeddingContext.train(self.wordEmbeddingContext.generate_training_data())\n",
    "        # wordEmbeddingContext.w1 now contains num_word row vectors, each of which has a size of settings[\"n\"]\n",
    "        \n",
    "        \n",
    "        self.count = []\n",
    "        self.np_Context = []\n",
    "        self.np_Query = []\n",
    "        self.Contexts_list = []\n",
    "        temp_count = 0\n",
    "        for context in data[\"data\"][0][\"paragraphs\"][0:1]:\n",
    "            # Create word2vec embedding for a Context\n",
    "            Context = self.tokenizeCorpus(context[\"context\"])  \n",
    "            self.Contexts_list.append({\"tokenized_context\": Context, \"context_string\": context[\"context\"]})\n",
    "            for word in Context:\n",
    "                # append a word vector from wordEmbeddingContext by taking the row of w1 at \"proper \"index. \n",
    "                # Also, w1 shape is (num_words, d = n) so we have to convert np.ndarray to list, so we can subsequently convert the list to tensor\n",
    "                inputContext.append(self.wordEmbeddingContext.w1[self.wordEmbeddingContext.getIndexFromWord(word)].tolist())\n",
    "            for q in range(len(Context), self.max_context_length):  # append zeros to the end of each context embedding to gain a coherent size of (max_context_words, n)\n",
    "                inputContext.append(np.zeros(self.settings[\"n\"]))\n",
    "            inputContext = np.asarray(inputContext, dtype = np.float32) # inputContext now is a matrix representation of the current context, and it has shape of (num_context_words, n)\n",
    "            # For each of queries about given Context, create word2vec embedding for that query\n",
    "            for query in context[\"qas\"]:\n",
    "                temp_count += 1\n",
    "                Query = self.tokenizeCorpus(query[\"question\"])\n",
    "                for word in Query:\n",
    "                    # append a word vector from wordEmbeddingContext by taking the row of w1 at \"proper \"index. \n",
    "                    # Also, w1 shape is (num_words, d = n) so we have to convert np.ndarray to list, so we can subsequently convert the list to tensor\n",
    "                    inputQuery.append(self.wordEmbeddingQuery.w1[self.wordEmbeddingQuery.wordIndex[word]].tolist()) # shape of inputQuery is (num_words, n)\n",
    "                for q in range(len(Query), self.max_query_length):# append zeros to the end of each query embedding to gain a coherent size of (max_query_words, n)\n",
    "                    inputQuery.append(np.zeros(self.settings[\"n\"]))\n",
    "                inputQuery = np.asarray(inputQuery, dtype = np.float32) # inputQuery now is a matrix representation of the current query, and it has shape of (num_query_words, n)\n",
    "                self.np_Query.append({\"question\": inputQuery, \"answer\": query[\"answers\"][0][\"text\"], \"query_text\": query[\"question\"]})\n",
    "                self.np_Context.append(inputContext)\n",
    "                inputQuery = [] # After appending one question matrix, clear it then append the next query matrices\n",
    "            inputContext = [] # After appending one context matrix, clear it then append the next context matrices\n",
    "            self.count.append(temp_count)\n",
    "            print(\"self.count is: \", self.count)\n",
    "            temp_count = 0\n",
    "        print(\"FINISHED EXTRACTING EMBEDDINGS FOR CONTEXT AND QUERIES, THEY RESPECTIVELY ARE OF SHAPES:\\n\\n\")\n",
    "        print(np.shape(self.np_Context))\n",
    "        print(np.shape(self.np_Query))\n",
    "        #np_Context = np.asarray(training_context_data, dtype = np.float32) # np_Context here is the context input training_data that will be passed to the fit function of Keras Model\n",
    "        #self.np_Context = np.transpose(np_Context, (0, 2, 1))\n",
    "        \n",
    "\n",
    "    def find_real_answer_indices(self, query_text, query_real_answer, context_index):\n",
    "        tokenized = word_tokenize(query_real_answer)\n",
    "        tokenized_answer = []\n",
    "        weirdlist = [\".\", \",\", \"'\",\"\\\"\", \"!\", \"?\", \"'\", \"-\", \"[\", \"]\", \":\", \"''\", \"``\", \")\", \"(\"]\n",
    "        for word in tokenized:\n",
    "            if word not in weirdlist:\n",
    "                tokenized_answer.append(word)\n",
    "            else:\n",
    "                continue\n",
    "        flag = True\n",
    "        save = 0\n",
    "        save1 = 0\n",
    "        for i in range(0, len(self.Contexts_list[context_index][\"tokenized_context\"])):\n",
    "            if self.Contexts_list[context_index][\"tokenized_context\"][i] == tokenized_answer[0]: # Compare the words in the context at index [context_index] in the self.Contexts_list with the words in the answer tokenized_answer\n",
    "                save = i\n",
    "                if len(tokenized_answer) > 1:\n",
    "                    for j in range(1, len(tokenized_answer)):\n",
    "                        if self.Contexts_list[context_index][\"tokenized_context\"][i+j] == tokenized_answer[j]:\n",
    "                            if j == len(tokenized_answer)-1:\n",
    "                                save1 = save + len(tokenized_answer)\n",
    "                                flag = False\n",
    "                                break\n",
    "                            else:\n",
    "                                continue\n",
    "                        else:\n",
    "                            break\n",
    "                    if flag:\n",
    "                        continue\n",
    "                    else:\n",
    "                        break        \n",
    "                    \n",
    "        start_index_real = save\n",
    "        end_index_real = save1\n",
    "        \n",
    "        return [[start_index_real, end_index_real]]\n",
    "        \n",
    "    def train(self, indices):\n",
    "        print(\"========================READY TO TRAIN NOW\\n\\n\\n\")\n",
    "        np_Context = np.array(self.np_Context)\n",
    "        print(\"np_Context is: \", np.shape(np_Context))\n",
    "        np_Query = []\n",
    "        for i in self.np_Query:\n",
    "            np_Query.append(i[\"question\"])\n",
    "        np_Query = np.array(np_Query)\n",
    "        print(\"np_Query is: \", np.shape(np_Query))\n",
    "        \n",
    "        print(\"indices is: \", np.shape(indices))\n",
    "        \n",
    "        \n",
    "        self.model.fit({\"context_input\": np_Context, \"query_input\": np_Query}, {\"output_indices\": indices}, epochs = self.settings[\"epochs\"])\n",
    "        \n",
    "def some_loss_function(real_answer_indices, prob_start_and_end):\n",
    "    print(\"\\n\\n___IN THE LOSS FUNCTION___\\nBelow is either called from Compiling phase or from Training phase, if in former phase, these information is abstract, else, it is specific instances\\n\\n\")\n",
    "    def compute_log_loss(true_and_pred):\n",
    "        real_answer_indices, p1_pred, p2_pred = true_and_pred\n",
    "        print(\"\\nTo compare with p1_pred before calling inner function conpute_log_loss, p1_pred is: \", p1_pred)\n",
    "        print(\"To compare with p2_pred before calling inner function conpute_log_loss, p2_pred is: \", p2_pred)\n",
    "        print(\"\\nIn inner function, real_answer_indices is now: \", tf.keras.backend.cast(real_answer_indices[0], dtype = 'int32'))\n",
    "        real_answer_indices_1 = tf.expand_dims(tf.keras.backend.cast(real_answer_indices[0][0][0], dtype = 'int32'), 0)\n",
    "        real_answer_indices_2 = tf.expand_dims(tf.keras.backend.cast(real_answer_indices[0][0][1], dtype = 'int32'), 0)\n",
    "        \n",
    "        if global_controller == 1:\n",
    "            sess = tf.compat.v1.InteractiveSession()\n",
    "            p1_pred.eval()\n",
    "            p2_pred.eval()\n",
    "            real_answer_indices_1.eval()\n",
    "            real_answer_indices_2.eval()\n",
    "        \n",
    "        print(\"indices 1:\", real_answer_indices_1)\n",
    "        start_prob = tf.gather(p1_pred, real_answer_indices_1)\n",
    "        end_prob = tf.gather(p2_pred, real_answer_indices_2)\n",
    "        \n",
    "        return -(tf.keras.backend.log(start_prob) + tf.keras.backend.log(end_prob))\n",
    "    \n",
    "    if global_controller == 1:\n",
    "        sess = tf.compat.v1.InteractiveSession()\n",
    "        prob_start_and_end\n",
    "       \n",
    "    print(\"\\n\\nprob_start_and_end is: \", prob_start_and_end)\n",
    "    print(\"Before inner function, real start: \", real_answer_indices[0][0])\n",
    "    print(\"\\t\\t\\treal end: \", real_answer_indices[0][0][1])\n",
    "    p1_pred = prob_start_and_end[0][0]    # since prob_start_and_end[0][0] is of size (200,), this when passed to compute_log_loss, will be decreased by 1 dimension, making its shape to be (), and \n",
    "    p2_pred = prob_start_and_end[0][1]\n",
    "                             \n",
    "    real_answer_indices_1 = tf.expand_dims(tf.keras.backend.cast(real_answer_indices[0][0][0], dtype = 'int32'), 0)\n",
    "    real_answer_indices_2 = tf.expand_dims(tf.keras.backend.cast(real_answer_indices[0][0][1], dtype = 'int32'), 0)\n",
    "    \n",
    "    start_prob = tf.gather(p1_pred, real_answer_indices_1)\n",
    "    end_prob = tf.gather(p2_pred, real_answer_indices_2)\n",
    "    \n",
    "    prob = -(tf.keras.backend.log(start_prob) + tf.keras.backend.log(end_prob))\n",
    "    \n",
    "    # print(\"\\nFrom some_loss_function, before calling conpute_log_loss, p1_pred is: \", p1_pred)\n",
    "    # print(\"From some_loss_function, before calling conpute_log_loss, p2_pred is: \", p2_pred)\n",
    "    # prob = tf.keras.backend.map_fn(compute_log_loss, (real_answer_indices, p1_pred, p2_pred), dtype = 'float32')\n",
    "    return tf.keras.backend.mean(prob, axis = 0)\n",
    "\n",
    "global_controller = 0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    myModel = MyQuestionAnsweringModel(120, 15)\n",
    "    indices = []\n",
    "    for i in range(0, len(myModel.count)):\n",
    "        for j in range(0, myModel.count[i]):\n",
    "            indices.append(myModel.find_real_answer_indices(myModel.np_Query[j][\"query_text\"], myModel.np_Query[j][\"answer\"], i))\n",
    "    global_controller = 1\n",
    "    myModel.train(np.asarray(indices))\n",
    "    tf.saved_model.save(myModel, \"System\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
